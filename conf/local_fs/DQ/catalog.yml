_spark_parquet: &spark_parquet
  type: kedro.contrib.io.pyspark.SparkDataSet
  file_format: "parquet"

dq_sampled_subscription_identifier:
  <<: *spark_parquet
  filepath: data/DQ/dq_sampled_subscription_identifier_journey/
  save_args:
    partitionBy: ["created_date"]
    mode: "append"

dq_accuracy_and_completeness:
  <<: *spark_parquet
  filepath: data/DQ/dq_accuracy_and_completeness/
  save_args:
    mode: "append"
    partitionBy: ["dataset_name", "corresponding_date"]

dq_availability:
  <<: *spark_parquet
  filepath: data/DQ/dq_availability/
  save_args:
    mode: "overwrite"
    partitionBy: ["dataset_name", "run_date"]

dq_consistency:
  <<: *spark_parquet
  filepath: data/DQ/dq_consistency/
  save_args:
    mode: "append"
    partitionBy: ["dataset_name", "run_date"]

dq_timeliness:
  <<: *spark_parquet
  filepath: data/DQ/dq_timeliness/
  save_args:
    mode: "overwrite"
    partitionBy: ["dataset_name", "run_date"]

dq_threshold_output_accuracy_and_completeness:
  <<: *spark_parquet
  filepath: data/DQ/dq_threshold_output_accuracy_and_completeness
  save_args:
    mode: "overwrite"
    partitionBy: ["dataset_name", "run_date"]

dq_threshold_output_accuracy_and_completeness_pivoted:
  <<: *spark_parquet
  filepath: data/DQ/dq_threshold_output_accuracy_and_completeness_pivoted
  save_args:
    mode: "overwrite"
    partitionBy: ["dataset_name", "run_date"]

dq_threshold_output_accuracy_and_completeness_grouped:
  <<: *spark_parquet
  filepath: data/DQ/dq_threshold_output_accuracy_and_completeness_grouped
  save_args:
    mode: "overwrite"
    partitionBy: ["run_date"]