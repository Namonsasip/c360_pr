# Here you can define all your data sets by using simple YAML syntax.
#
# Documentation for this file format can be found in the kedro docs under `Accessing data`
# You can access the kedro docs by running `kedro docs`

#
# An example data set definition can look as follows:
#
#cars.csv:
#  type: CSVLocalDataSet # https://kedro.readthedocs.io/en/latest/kedro.io.CSVLocalDataSet.html
#  filepath: data/L0/company/cars.csv
#  load_args: # https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html
#    sep: ','
#    skiprows: 0
#    # skipfooter: 1
#    # engine: python  # Some of the features including skipfooter is only available in python engine
#    engine: c  # This is a faster option
#    na_values: ['#NA', 'NA']
#  save_args: # https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.to_csv.html
#    index: False
#    date_format: '%Y-%m-%d %H:%M'
#    decimal: '.'
#
#cars.csv.s3:
#  type: CSVS3DataSet # https://kedro.readthedocs.io/en/latest/kedro.io.CSVS3DataSet.html
#  filepath: data/L1/company/cars.csv
#  bucket_name: my_bucket
#  credentials: dev_s3
#  load_args: # https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html
#    sep: ','
#    skiprows: 5
#    skipfooter: 1
#    na_values: ['#NA', 'NA']
#    index: False
#  save_args: # https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.to_csv.html
#    index: False
#    date_format: '%Y-%m-%d %H:%M'
#    decimal: '.'
#
#cars.hdf:
#  type: HDFLocalDataSet  # https://kedro.readthedocs.io/en/latest/kedro.io.HDFLocalDataSet.html
#  filepath: data/L1/cars.hdf
#  key: name
#  load_args:  # https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_hdf.html
#    columns: ['engine', 'name']
#  save_args:  # https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.to_hdf.html
#    mode: 'w'  # Overwrite even when the file already exists
#    # mode: 'a'  # Appends or creates a file
#    # mode: '+r'  # Appends an existing file
#    dropna: True
#
#cars.hdf.s3:
#  type: HDFS3DataSet  # https://kedro.readthedocs.io/en/latest/kedro.io.HDFS3DataSet.html
#  filepath: data/L1/cars.hdf
#  bucket_name: my_bucket
#  key: hdf_key
#  credentials: dev_s3
#  load_args:
#  save_args:
#
#cars.parquet:
#  type: ParquetLocalDataSet  # https://kedro.readthedocs.io/en/latest/kedro.io.ParquetLocalDataSet.html
#  filepath: data/L1/cars.parquet
#  load_args:
#    columns: ['name', 'gear','disp', 'wt']
#  save_args:
#     compression: 'GZIP'
#
#cars.sql:
#  type: SQLTableDataSet  # https://kedro.readthedocs.io/en/latest/kedro.io.SQLTableDataSet.html
#  credentials: dev_postgres
#  table_name: cars
#  load_args:  # https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_sql_table.html
#    index_col: ['name']
#    columns: ['name', 'gear']
#  save_args:  # https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.to_sql.html
#    if_exists: 'replace'
#    # if_exists: 'fail'
#    # if_exists: 'append'
#
#cars.sql.query:
#  type: SQLQueryDataSet  # https://kedro.readthedocs.io/en/latest/kedro.io.SQLQueryDataSet.html
#  credentials: dev_postgres
#  sql: 'select * from cars where gear=4'
#  load_args:  # https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_sql_query.html
#    index_col: ['name']
#
#car_model.pkl:
#  type: PickleLocalDataSet
#  filepath: data/06_models/car_model.pkl
#  backend: pickle
#
## Templating and reuse
#
#_csv: &csv
#  type: kedro.contrib.io.pyspark.spark_data_set.SparkDataSet
#  file_format: 'csv'
#  load_args:
#    header: True
#    inferSchema: False
#
#raw_banana_trials:
#  <<: *csv
#  filepath: "s3a://supermarket/L0/Banana/trials.csv"
#
## Transcoding
#
#cars@parquet:
#  type: ParquetLocalDataSet  # https://kedro.readthedocs.io/en/latest/kedro.io.ParquetLocalDataSet.html
#  filepath: data/L1/cars.parquet
#  load_args:
#    columns: ['name', 'gear','disp', 'wt']
#  save_args:
#     compression: 'GZIP'
#
#cars@spark:
#  type: kedro.contrib.io.pyspark.SparkDataSet  # https://kedro.readthedocs.io/en/latest/kedro.contrib.io.pyspark.SparkDataSet.html
#  filepath: data/L1/cars.parquet



# This is a data set used by the example pipeline provided with the projected
# template. Please feel free to remove it once you remove the example pipeline.

l1_billing_and_payments_daily_topup_and_volume:
  type: kedro.contrib.io.pyspark.SparkDataSet
  file_format: "parquet"
  filepath: data/L1_DAILY/BILLING/billing_and_payments_topup_and_volume/
  save_args:
    mode: "overwrite"
    partitionBy: ["event_partition_date"]

l1_billing_and_payments_daily_rpu_roaming:
  type: kedro.contrib.io.pyspark.SparkDataSet
  file_format: "parquet"
  filepath: data/L1_DAILY/BILLING/billing_and_payments_rpu_roaming/
  save_args:
    partitionBy: ["event_partition_date"]
    mode: "overwrite"

l1_billing_and_payments_daily_before_top_up_balance:
  type: kedro.contrib.io.pyspark.SparkDataSet
  file_format: "parquet"
  filepath: data/L1_DAILY/BILLING/billing_and_payments_before_top_up_balance/
  save_args:
    partitionBy: ["event_partition_date"]
    mode: "overwrite"

l1_billing_and_payments_daily_top_up_channels:
  type: kedro.contrib.io.pyspark.SparkDataSet
  file_format: "parquet"
  filepath: data/L1_DAILY/BILLING/billing_and_payments_top_up_channels/
  save_args:
    partitionBy: ["event_partition_date"]
    mode: "overwrite"


l1_billing_and_payments_daily_most_popular_top_up_channel:
  type: kedro.contrib.io.pyspark.SparkDataSet
  file_format: "parquet"
  filepath: data/L1_DAILY/BILLING/billing_and_payments_most_popular_top_up_channel/
  save_args:
    partitionBy: ["event_partition_date"]
    mode: "overwrite"

l1_billing_and_payments_daily_popular_topup_day:
  type: kedro.contrib.io.pyspark.SparkDataSet
  file_format: "parquet"
  filepath: data/L1_DAILY/BILLING/billing_and_payments_popular_topup_day/
  save_args:
    partitionBy: ["event_partition_date"]
    mode: "overwrite"

l1_billing_and_payments_daily_time_since_last_top_up:
  type: kedro.contrib.io.pyspark.SparkDataSet
  file_format: "parquet"
  filepath: data/L1_DAILY/BILLING/billing_and_payments_time_since_last_top_up/
  save_args:
    partitionBy: ["event_partition_date"]
    mode: "overwrite"