{"cells":[{"cell_type":"code","source":["from pyspark.sql import SparkSession\nfrom pyspark.sql import functions as F"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":1},{"cell_type":"code","source":["def _get_spark():\n  spark = SparkSession.builder.appName(\"project_customer_360_campaign_test\").getOrCreate()\n  spark.conf.set(\"spark.sql.sources.partitionOverwriteMode\", \"dynamic\")\n  spark.conf.set(\"spark.sql.parquet.binaryAsString\", \"true\")\n  spark.conf.set(\"spark.sql.shuffle.partitions\", 200)\n  spark.conf.set(\"spark.sql.files.maxPartitionBytes\", 1024*1024*256)\n  return spark\nspark = _get_spark()\nspark.conf.set(\"spark.sql.sources.partitionOverwriteMode\", \"dynamic\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":2},{"cell_type":"code","source":["# Below Command are executed only once to mount the blob as a mount volume on Databricks\n# Mount Points:\n# Blob Storage customer360-blob-data is available at mount point /mnt/customer360-blob-data\n# Blob Storage customer360-blob-output is available at mount point /mnt/customer360-blob-output\nimport pandas as pd\npd.set_option('display.max_rows', None)\npd.set_option('display.max_columns', None)\n\nfrom pyspark.sql.functions import *\n\nimport sys\nsys.version_info\nspark.conf.set(\"spark.sql.parquet.binaryAsString\",\"true\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":3},{"cell_type":"code","source":["spark.conf.set(\"spark.sql.sources.partitionOverwriteMode\", \"DYNAMIC\")\nimport datetime\n\ndt = datetime.datetime(2019, 8, 1)\nend = datetime.datetime(2020, 2, 14)\nstep = datetime.timedelta(days=1)\n\nresult = []\ndf = spark.read.parquet(\"dbfs:/mnt/customer360-blob-output/C360/PROFILE/l1_features/l1_customer_profile_union_daily_feature/event_partition_date=2020-02-14/\")\n\nwhile dt < end:\n  partition_date = dt.strftime('%Y-%m-%d')\n  result.append(partition_date)\n  dt += step\n  # only save whatever active before or at current date\n  filtered_df = df.filter(col(\"register_date\") <= to_date(lit(partition_date)))\\\n                          .withColumn(\"event_partition_date\", to_date(lit(partition_date))) \n\n  (filtered_df.write\n   .partitionBy(\"event_partition_date\")\n   .mode(\"overwrite\")\n   .parquet(\"dbfs:/mnt/customer360-blob-output/C360/PROFILE/l1_features/l1_customer_profile_union_daily_feature/\"))\n\n  print(partition_date)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">2019-08-01\n2019-08-02\n2019-08-03\n2019-08-04\n2019-08-05\n2019-08-06\n2019-08-07\n2019-08-08\n2019-08-09\n2019-08-10\n2019-08-11\n2019-08-12\n2019-08-13\n2019-08-14\n2019-08-15\n2019-08-16\n2019-08-17\n2019-08-18\n2019-08-19\n2019-08-20\n2019-08-21\n2019-08-22\n2019-08-23\n2019-08-24\n2019-08-25\n2019-08-26\n2019-08-27\n2019-08-28\n2019-08-29\n2019-08-30\n2019-08-31\n2019-09-01\n2019-09-02\n2019-09-03\n2019-09-04\n2019-09-05\n2019-09-06\n2019-09-07\n2019-09-08\n2019-09-09\n2019-09-10\n2019-09-11\n2019-09-12\n2019-09-13\n2019-09-14\n2019-09-15\n2019-09-16\n2019-09-17\n2019-09-18\n2019-09-19\n2019-09-20\n2019-09-21\n2019-09-22\n2019-09-23\n2019-09-24\n2019-09-25\n2019-09-26\n2019-09-27\n2019-09-28\n2019-09-29\n2019-09-30\n2019-10-01\n2019-10-02\n2019-10-03\n2019-10-04\n2019-10-05\n2019-10-06\n2019-10-07\n2019-10-08\n2019-10-09\n2019-10-10\n2019-10-11\n2019-10-12\n2019-10-13\n2019-10-14\n2019-10-15\n2019-10-16\n2019-10-17\n2019-10-18\n2019-10-19\n2019-10-20\n2019-10-21\n2019-10-22\n2019-10-23\n2019-10-24\n2019-10-25\n2019-10-26\n2019-10-27\n2019-10-28\n2019-10-29\n2019-10-30\n2019-10-31\n2019-11-01\n2019-11-02\n2019-11-03\n2019-11-04\n2019-11-05\n2019-11-06\n2019-11-07\n2019-11-08\n2019-11-09\n2019-11-10\n2019-11-11\n2019-11-12\n2019-11-13\n2019-11-14\n2019-11-15\n2019-11-16\n2019-11-17\n2019-11-18\n2019-11-19\n2019-11-20\n2019-11-21\n2019-11-22\n2019-11-23\n2019-11-24\n2019-11-25\n2019-11-26\n2019-11-27\n2019-11-28\n2019-11-29\n2019-11-30\n2019-12-01\n2019-12-02\n2019-12-03\n2019-12-04\n2019-12-05\n2019-12-06\n2019-12-07\n2019-12-08\n2019-12-09\n2019-12-10\n2019-12-11\n2019-12-12\n2019-12-13\n2019-12-14\n2019-12-15\n2019-12-16\n2019-12-17\n2019-12-18\n2019-12-19\n2019-12-20\n2019-12-21\n2019-12-22\n2019-12-23\n2019-12-24\n2019-12-25\n2019-12-26\n2019-12-27\n2019-12-28\n2019-12-29\n2019-12-30\n2019-12-31\n2020-01-01\n2020-01-02\n2020-01-03\n2020-01-04\n2020-01-05\n2020-01-06\n2020-01-07\n2020-01-08\n2020-01-09\n2020-01-10\n2020-01-11\n2020-01-12\n2020-01-13\n2020-01-14\n2020-01-15\n2020-01-16\n2020-01-17\n2020-01-18\n2020-01-19\n2020-01-20\n2020-01-21\n2020-01-22\n2020-01-23\n2020-01-24\n2020-01-25\n2020-01-26\n2020-01-27\n2020-01-28\n2020-01-29\n2020-01-30\n2020-01-31\n2020-02-01\n2020-02-02\n2020-02-03\n2020-02-04\n2020-02-05\n2020-02-06\n2020-02-07\n2020-02-08\n2020-02-09\n2020-02-10\n2020-02-11\n2020-02-12\n2020-02-13\n</div>"]}}],"execution_count":4},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":5}],"metadata":{"name":"backfill_L1_customer_profile","notebookId":3539511345471550},"nbformat":4,"nbformat_minor":0}
