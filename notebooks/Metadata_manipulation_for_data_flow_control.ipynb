{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the varaible based on your environment\n",
    "# On cloud\n",
    "metadata_table_path = '/mnt/customer360-blob-output/C360/UTILITIES/metadata_table'\n",
    "# On premise\n",
    "# metadata_table_path = '/projects/prod/c360/data/UTILITIES/metadata_table'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Before running this notebook, make sure to create a backup of metadata table.\n",
    "\n",
    "# import datetime\n",
    "# now = datetime.datetime.now()\n",
    "# bkp_path = metadata_table_path+now.strftime(\"%Y_%m_%d_%H_%M_%S\")+'/metadata_table_bkp/'\n",
    "#dbutils.fs.cp(metadata_table_path, bkp_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################### Daily level (L1 and  L1-L4 layer) datasets #######################\n",
    "# In this scenario, we need to put the reset data as (corrupt_date - 1) day, where corrupt_date is the minimum of date(s) for which corrupt data is loaded.\n",
    "\n",
    "# For e.g\n",
    "# Let's say, for dataset - D1, we have found out that corrupt/dirty data got loaded from 24th April (24/04/2020) till 26th April (26/04/2020).\n",
    "# Then the \"reset_date_for_dataset\" will be '23rd April' (23/04/2020)\n",
    "\n",
    "# Because this is the date till which correct data had already been processed.\n",
    "\n",
    "\n",
    "\n",
    "####################### Weekly level (L2 and L2-L4 layer) datasets #######################\n",
    "# In this scenario, we need to put the reset data as \"start of previous week\", where corrupt_date is the minimum of week(s) for which corrupt data is loaded.\n",
    "\n",
    "# For e.g\n",
    "# Let's say, for dataset - D2, we have found out that corrupt/dirty data got loaded for week of 20th April (20/04/2020) and 27th April (27/04/2020).\n",
    "# Then the \"reset_date_for_dataset\" will be '13rd April' (13/04/2020)\n",
    "\n",
    "# Because this is the week till which correct data had already been processed.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "####################### Monthly level (L3 and L3-L4 layer) datasets #######################\n",
    "# In this scenario, we need to put the reset data as \"start of previous month\", where corrupt_date is the minimum of months(s) for which corrupt data is loaded.\n",
    "\n",
    "# For e.g\n",
    "# Let's say, for dataset - D3, we have found out that corrupt/dirty data got loaded from 01st April (01/04/2020) and 01st May (01/05/2020).\n",
    "# Then the \"reset_date_for_dataset\" will be '01st Mar' (01/03/2020)\n",
    "\n",
    "# Because this is the month till which correct data had already been processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_name_for_reset = 'test'\n",
    "dataset_path_for_reset = '/mnt/customer360-blob-output/C360/REVENUE/l4_features/test/'\n",
    "reset_date_for_dataset = '1970-01-01'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def _get_spark():\n",
    "  spark = SparkSession.builder.appName(\"project_customer_360_data_flow_control\").getOrCreate()\n",
    "  spark.conf.set(\"spark.sql.sources.partitionOverwriteMode\", \"dynamic\")\n",
    "  spark.conf.set(\"spark.sql.parquet.binaryAsString\", \"true\")\n",
    "  spark.conf.set(\"spark.sql.shuffle.partitions\", 200)\n",
    "  spark.conf.set(\"spark.sql.files.maxPartitionBytes\", 1024*1024*256)\n",
    "  return spark\n",
    "spark = _get_spark()\n",
    "spark.conf.set(\"spark.sql.sources.partitionOverwriteMode\", \"dynamic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mtdt_tbl = spark.read.parquet(metadata_table_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "metadata_table_update_df = spark.range(1)\n",
    "\n",
    "metadata_table_update_df = (\n",
    "            metadata_table_update_df.withColumn(\"table_name\", F.lit(dataset_name_for_reset))\n",
    "                .withColumn(\"table_path\", F.lit(dataset_path_for_reset))\n",
    "                .withColumn(\"write_mode\", F.lit(\"overwrite\"))\n",
    "                .withColumn(\"target_max_data_load_date\", F.to_date(F.lit(reset_date_for_dataset), \"yyyy-MM-dd\"))\n",
    "                .withColumn(\"updated_on\", F.current_date())\n",
    "                .drop(\"id\")\n",
    "                    )\n",
    "\n",
    "metadata_table_update_df.write.partitionBy(\"table_name\").format(\"parquet\").mode(\"overwrite\").save(metadata_table_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "name": "Metadata_manipulation_for_data_flow_control",
  "notebookId": 1151079605104122
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
